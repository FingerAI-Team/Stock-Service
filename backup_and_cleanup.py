#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ë°±ì—… ë° ë°ì´í„°ë² ì´ìŠ¤ ì •ë¦¬ ìŠ¤í¬ë¦½íŠ¸
1. ê¸°ì¡´ ë°ì´í„° ë°±ì—…
2. hash_valueê°€ ì—†ëŠ” ë°ì´í„°ì— í•´ì‹œê°’ ìƒì„±
3. conv_idì— hash_valueê°€ í¬í•¨ëœ ì˜ëª»ëœ ë°ì´í„° ì œê±°
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from src import EnvManager, DBManager
import argparse
import logging
import hashlib
import pandas as pd
from datetime import datetime
from tqdm import tqdm

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("backup_cleanup.log"),
    ]
)

def generate_hash_value(user_id, content, date):
    """í•´ì‹œê°’ ìƒì„±"""
    return hashlib.md5(
        f"{user_id}_{content}_{date}".encode()
    ).hexdigest()

def backup_data(postgres, table_name, backup_file):
    """ë°ì´í„° ë°±ì—…"""
    logger = logging.getLogger(__name__)
    logger.info(f"ğŸ’¾ ë°ì´í„° ë°±ì—… ì¤‘... ({backup_file})")
    
    # ì „ì²´ ë°ì´í„° ì¡°íšŒ
    postgres.db_connection.cur.execute(f"SELECT * FROM {table_name}")
    columns = [desc[0] for desc in postgres.db_connection.cur.description]
    data = postgres.db_connection.cur.fetchall()
    
    # DataFrameìœ¼ë¡œ ë³€í™˜í•˜ì—¬ CSVë¡œ ì €ì¥
    df = pd.DataFrame(data, columns=columns)
    df.to_csv(backup_file, index=False, encoding='utf-8')
    
    logger.info(f"âœ… ë°±ì—… ì™„ë£Œ: {len(data)}ê°œ ë ˆì½”ë“œ â†’ {backup_file}")

def cleanup_database_with_backup():
    """ë°±ì—… í›„ ë°ì´í„°ë² ì´ìŠ¤ ì •ë¦¬ ì‘ì—…"""
    logger = logging.getLogger(__name__)
    
    # í™˜ê²½ ì„¤ì •
    args = argparse.Namespace()
    args.config_path = './config/'
    env_manager = EnvManager(args)
    db_manager = DBManager(env_manager.db_config)
    
    # ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°
    postgres, table_editor = db_manager.initialize_database()
    table_name = env_manager.conv_tb_name
    
    # ë°±ì—… íŒŒì¼ëª… ìƒì„±
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    backup_file = f"backup_{table_name}_{timestamp}.csv"
    
    logger.info("ğŸ” ë°ì´í„°ë² ì´ìŠ¤ ì •ë¦¬ ì‘ì—… ì‹œì‘")
    
    # 1. ë°±ì—… ìƒì„±
    backup_data(postgres, table_name, backup_file)
    
    # 2. hash_valueê°€ ì—†ëŠ” ë°ì´í„° ì°¾ê¸°
    logger.info("ğŸ“‹ hash_valueê°€ ì—†ëŠ” ë°ì´í„° ì¡°íšŒ ì¤‘...")
    postgres.db_connection.cur.execute(
        f"SELECT conv_id, user_id, content, date FROM {table_name} WHERE hash_value IS NULL"
    )
    null_hash_records = postgres.db_connection.cur.fetchall()
    
    logger.info(f"âœ… hash_valueê°€ ì—†ëŠ” ë ˆì½”ë“œ: {len(null_hash_records)}ê°œ")
    
    # hash_value ì—…ë°ì´íŠ¸
    if null_hash_records:
        logger.info("ğŸ”„ hash_value ìƒì„± ë° ì—…ë°ì´íŠ¸ ì¤‘...")
        updated_count = 0
        
        for record in tqdm(null_hash_records, desc="í•´ì‹œê°’ ì—…ë°ì´íŠ¸"):
            conv_id, user_id, content, date = record
            
            # í•´ì‹œê°’ ìƒì„±
            hash_value = generate_hash_value(user_id, content, date)
            
            # ì—…ë°ì´íŠ¸
            postgres.db_connection.cur.execute(
                f"UPDATE {table_name} SET hash_value = %s WHERE conv_id = %s",
                (hash_value, conv_id)
            )
            updated_count += 1
        
        postgres.db_connection.conn.commit()
        logger.info(f"âœ… {updated_count}ê°œ ë ˆì½”ë“œì˜ hash_value ì—…ë°ì´íŠ¸ ì™„ë£Œ")
    
    # 3. conv_idì— hash_valueê°€ í¬í•¨ëœ ì˜ëª»ëœ ë°ì´í„° ì°¾ê¸°
    logger.info("ğŸ” conv_idì— í•´ì‹œê°’ì´ í¬í•¨ëœ ì˜ëª»ëœ ë°ì´í„° ì¡°íšŒ ì¤‘...")
    
    # í•´ì‹œê°’ íŒ¨í„´ì„ ê°€ì§„ conv_id ì°¾ê¸° (ì˜ˆ: 20250127_user123_abc12345)
    postgres.db_connection.cur.execute(
        f"SELECT conv_id FROM {table_name} WHERE conv_id ~ '^[0-9]{{8}}_[a-zA-Z0-9]+_[a-f0-9]{{8}}$'"
    )
    wrong_conv_ids = postgres.db_connection.cur.fetchall()
    
    logger.info(f"âš ï¸ ì˜ëª»ëœ í˜•ì‹ì˜ conv_id: {len(wrong_conv_ids)}ê°œ")
    
    # ì˜ëª»ëœ ë°ì´í„° ì œê±°
    if wrong_conv_ids:
        logger.info("ğŸ—‘ï¸ ì˜ëª»ëœ í˜•ì‹ì˜ ë°ì´í„° ì œê±° ì¤‘...")
        deleted_count = 0
        
        for (conv_id,) in tqdm(wrong_conv_ids, desc="ë°ì´í„° ì œê±°"):
            postgres.db_connection.cur.execute(
                f"DELETE FROM {table_name} WHERE conv_id = %s",
                (conv_id,)
            )
            deleted_count += 1
        
        postgres.db_connection.conn.commit()
        logger.info(f"âœ… {deleted_count}ê°œ ì˜ëª»ëœ ë ˆì½”ë“œ ì œê±° ì™„ë£Œ")
    
    # 4. ì¤‘ë³µ ë°ì´í„° í™•ì¸ (hash_value ê¸°ì¤€)
    logger.info("ğŸ” hash_value ê¸°ì¤€ ì¤‘ë³µ ë°ì´í„° í™•ì¸ ì¤‘...")
    postgres.db_connection.cur.execute(
        f"""
        SELECT hash_value, COUNT(*) as count 
        FROM {table_name} 
        WHERE hash_value IS NOT NULL 
        GROUP BY hash_value 
        HAVING COUNT(*) > 1
        """
    )
    duplicate_hashes = postgres.db_connection.cur.fetchall()
    
    if duplicate_hashes:
        logger.warning(f"âš ï¸ ì¤‘ë³µëœ hash_value: {len(duplicate_hashes)}ê°œ")
        for hash_value, count in duplicate_hashes:
            logger.warning(f"   í•´ì‹œ {hash_value[:8]}...: {count}ê°œ ì¤‘ë³µ")
    else:
        logger.info("âœ… ì¤‘ë³µ ë°ì´í„° ì—†ìŒ")
    
    # 5. ìµœì¢… í†µê³„
    postgres.db_connection.cur.execute(f"SELECT COUNT(*) FROM {table_name}")
    total_count = postgres.db_connection.cur.fetchone()[0]
    
    postgres.db_connection.cur.execute(
        f"SELECT COUNT(*) FROM {table_name} WHERE hash_value IS NOT NULL"
    )
    hash_count = postgres.db_connection.cur.fetchone()[0]
    
    logger.info("ğŸ“Š ìµœì¢… í†µê³„:")
    logger.info(f"   ì „ì²´ ë ˆì½”ë“œ: {total_count}")
    logger.info(f"   hash_value ìˆëŠ” ë ˆì½”ë“œ: {hash_count}")
    logger.info(f"   hash_value ì—†ëŠ” ë ˆì½”ë“œ: {total_count - hash_count}")
    logger.info(f"   ë°±ì—… íŒŒì¼: {backup_file}")
    
    # ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì¢…ë£Œ
    postgres.db_connection.close()
    logger.info("ğŸ‰ ë°ì´í„°ë² ì´ìŠ¤ ì •ë¦¬ ì‘ì—… ì™„ë£Œ")

if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='ë°±ì—… ë° ë°ì´í„°ë² ì´ìŠ¤ ì •ë¦¬ ìŠ¤í¬ë¦½íŠ¸')
    parser.add_argument('--execute', action='store_true',
                       help='ë°±ì—… í›„ ì‹¤ì œ ì •ë¦¬ ì‘ì—… ì‹¤í–‰')
    
    args = parser.parse_args()
    
    if args.execute:
        cleanup_database_with_backup()
    else:
        print("ì‚¬ìš©ë²•:")
        print("  python backup_and_cleanup.py --execute   # ë°±ì—… í›„ ì •ë¦¬ ì‘ì—… ì‹¤í–‰")
        print("")
        print("âš ï¸ ì£¼ì˜ì‚¬í•­:")
        print("  - ì´ ì‘ì—…ì€ ë°ì´í„°ë¥¼ ë³€ê²½í•©ë‹ˆë‹¤")
        print("  - ì‹¤í–‰ ì „ ìë™ìœ¼ë¡œ ë°±ì—…ì´ ìƒì„±ë©ë‹ˆë‹¤")
        print("  - ë°±ì—… íŒŒì¼ì€ CSV í˜•ì‹ìœ¼ë¡œ ì €ì¥ë©ë‹ˆë‹¤")
