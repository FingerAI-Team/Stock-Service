from .preprocessor import DataProcessor, TextProcessor, VecProcessor, TimeProcessor
from .encoder import KFDeBERTaTokenizer, KFDeBERTa, ModelTrainer, ModelPredictor
from .database import PostgresDB, DBConnection, TableEditor
from .llm import LLMOpenAI
from datasets import Dataset, DatasetDict
from dotenv import load_dotenv
from tqdm import tqdm
import pandas as pd
import time
import json
import os
import requests
import logging
from datetime import datetime, timezone, timedelta


class EnvManager:
    def __init__(self, args):
        load_dotenv()
        self.args = args 
        self.openai_api_key = os.getenv('OPENAI_API_KEY')
        self.bearer_token = os.getenv("BEARER_TOKEN")
        if not self.openai_api_key:
            raise ValueError("OpenAI API í‚¤ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        self.tickle_list = self.__load_tickle_list()
        self.model_config, self.db_config = self.__load_configs()
        self.conv_tb_name, self.cls_tb_name, self.clicked_tb_name = 'ibk_convlog', 'ibk_stock_cls', 'ibk_clicked_tb'   

    def __load_configs(self):
        '''
        llm ëª¨ë¸ê³¼ db ì„¤ì • íŒŒì¼ì„ ë¡œë“œí•©ë‹ˆë‹¤.
        '''
        with open(os.path.join(self.args.config_path, 'llm_config.json')) as f:
            llm_config = json.load(f)
 
        with open(os.path.join(self.args.config_path, 'db_config.json')) as f:
            db_config = json.load(f)
        return llm_config, db_config

    def __load_tickle_list(self):
        '''
        êµ­ë‚´ ì¦ê¶Œ ì¢…ëª© & í•´ì™¸ ì¦ê¶Œ ì¢…ëª© ë¦¬ìŠ¤íŠ¸ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤. 
        returns:
        list[str]: ì¦ê¶Œ ì¢…ëª© ì´ë¦„, ì½”ë“œ ê°’
        '''
        tickles = pd.read_csv(os.path.join('./', 'tickle', 'tickle-final.csv'))
        tickles.dropna(inplace=True)
        return tickles['tickle'].values.tolist()

class PreProcessor:
    def initialize_processor(self):
        return DataProcessor(), TextProcessor(), VecProcessor(), TimeProcessor()
        
class DBManager:
    def __init__(self, db_config):
        self.db_config = db_config

    def initialize_database(self):
        db_connection = DBConnection(self.db_config)
        db_connection.connect()
        postgres = PostgresDB(db_connection)
        table_editor = TableEditor(db_connection)
        return postgres, table_editor


class APIPipeline:
    def __init__(self, bearer_tok):
        self.bearer_tok = bearer_tok 

    def get_data_range(self, start_date, end_date, tenant_id='ibk'):
        url = f"https://chat-api.ibks.onelineai.com/api/ibk_securities/admin/logs?tenant_id={tenant_id}"
        # ë‹¤ìŒ ë‚ ì„ to_dateë¡œ ì„¤ì • (get_data_api.pyì™€ ë™ì¼í•œ ë°©ì‹)
        request_url = f"{url}&from_date_utc={start_date}&to_date_utc={end_date}"
        headers = {
            "Authorization": f"Bearer {self.bearer_tok}"
        }
        print(f"API ìš”ì²­ URL: {request_url}")
        print(f"Bearer Token: {self.bearer_tok[:10]}..." if self.bearer_tok else "Bearer Token ì—†ìŒ")
        try:
            response = requests.get(request_url, headers=headers)
            print(f"API ì‘ë‹µ ìƒíƒœ ì½”ë“œ: {response.status_code}")
            
            if response.status_code == 200:
                data = response.json()
                print(f"API ì‘ë‹µ ë°ì´í„° íƒ€ì…: {type(data)}")
                return data
            else:
                print(f"API ìš”ì²­ ì‹¤íŒ¨: {response.status_code} - {response.text}")
                return []
                
        except Exception as e:
            print(f"API ìš”ì²­ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            return []
    
    def get_data(self, date, tenant_id='ibk'):
        url = f"https://chat-api.ibks.onelineai.com/api/ibk_securities/admin/logs?tenant_id={tenant_id}"
        # ë‹¤ìŒ ë‚ ì„ to_dateë¡œ ì„¤ì • (get_data_api.pyì™€ ë™ì¼í•œ ë°©ì‹)
        from datetime import datetime, timedelta
        from_date = datetime.strptime(date, "%Y-%m-%d")
        end_date = from_date + timedelta(days=1)
        request_url = f"{url}&from_date_utc={from_date}&to_date_utc={end_date}"
        headers = {
            "Authorization": f"Bearer {self.bearer_tok}"
        }
        print(f"API ìš”ì²­ URL: {request_url}")
        print(f"Bearer Token: {self.bearer_tok[:10]}..." if self.bearer_tok else "Bearer Token ì—†ìŒ")
        try:
            response = requests.get(request_url, headers=headers)
            print(f"API ì‘ë‹µ ìƒíƒœ ì½”ë“œ: {response.status_code}")
            
            if response.status_code == 200:
                data = response.json()
                print(f"API ì‘ë‹µ ë°ì´í„° íƒ€ì…: {type(data)}")
                return data
            else:
                print(f"API ìš”ì²­ ì‹¤íŒ¨: {response.status_code} - {response.text}")
                return []
        except Exception as e:
            print(f"API ìš”ì²­ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            return []
    
    def process_data(self, data):
        '''
        apië¡œ ë°›ì€ ë°ì´í„°ë¥¼ postgres dbì— ì €ì¥ ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë³€ê²½  
        date, qa, content, user_id, tenant_id, hash_value, hash_ref: ibk, msty 
        '''
        if not data:
            print("APIì—ì„œ ë°›ì€ ë°ì´í„°ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
            return pd.DataFrame(columns=["date", "q/a", "content", "user_id", "tenant_id", "hash_value", "hash_ref"])
            
        import hashlib
        records = []
        for d in data:
            if "Q" in d and "A" in d and "date" in d and "user_id" in d:
                # user_idê°€ Noneì¸ ê²½ìš° ì²˜ë¦¬
                user_id = d["user_id"] if d["user_id"] is not None else ""
                tenant_id = d.get("tenant_id") if d.get("tenant_id") is not None else None
                
                # Qì™€ Aì˜ í•´ì‹œê°’ì„ ë¯¸ë¦¬ ìƒì„± (user_idê°€ Noneì´ë©´ ë¹ˆ ë¬¸ìì—´ ì‚¬ìš©)
                q_hash = hashlib.md5(f"{user_id}_{d['Q']}_{d['date']}".encode()).hexdigest()
                a_hash = hashlib.md5(f"{user_id}_{d['A']}_{d['date']}".encode()).hexdigest()
                
                records.append({
                    "date": d["date"], 
                    "q/a": "Q", 
                    "content": d["Q"], 
                    "user_id": d["user_id"],  # None ê°’ë„ ê·¸ëŒ€ë¡œ ì €ì¥ (DBì—ì„œ nullable)
                    "tenant_id": tenant_id,
                    "hash_value": q_hash,
                    "hash_ref": None  # QëŠ” hash_refê°€ NULL
                })
                records.append({
                    "date": d["date"], 
                    "q/a": "A", 
                    "content": d["A"], 
                    "user_id": d["user_id"],  # None ê°’ë„ ê·¸ëŒ€ë¡œ ì €ì¥ (DBì—ì„œ nullable)
                    "tenant_id": tenant_id,
                    "hash_value": a_hash,
                    "hash_ref": q_hash  # AëŠ” Qì˜ hash_valueë¥¼ hash_refë¡œ
                })
            else:
                print(f"ë°ì´í„° êµ¬ì¡°ê°€ ì˜ˆìƒê³¼ ë‹¤ë¦…ë‹ˆë‹¤: {d.keys()}")
        if not records:
            print("ì²˜ë¦¬ ê°€ëŠ¥í•œ ë ˆì½”ë“œê°€ ì—†ìŠµë‹ˆë‹¤.")
            return pd.DataFrame(columns=["date", "q/a", "content", "user_id", "tenant_id", "hash_value", "hash_ref"])
            
        input_data = pd.DataFrame(records, columns=["date", "q/a", "content", "user_id", "tenant_id", "hash_value", "hash_ref"])
        print(f"ì²˜ë¦¬ëœ ë ˆì½”ë“œ ìˆ˜: {len(input_data)}")
        return input_data


class ModelManager:
    def __init__(self, model_config):
        self.model_config = model_config
   
    def set_cls_trainset(self, dataset, dataset2, data_processor):
        '''
        dbì—ì„œ ì…ë ¥ë°›ì€ ë°ì´í„°ë¥¼ í•™ìŠµ ë°ì´í„°ì„¸íŠ¸ë¡œ ë³€í™˜í•©ë‹ˆë‹¤. 
        '''
        convlog_data = data_processor.data_to_df(dataset, columns=['conv_id', 'date', 'qa', 'content', 'userid'])
        cls_data = data_processor.data_to_df(dataset2, columns=['conv_id', 'ensemble', 'gpt', 'encoder']) 
        convlog_q = data_processor.filter_data(convlog_data, 'qa', 'Q')
        convlog_trainset = data_processor.merge_data(convlog_q, cls_data, on='conv_id')
        convlog_trainset['label'] = convlog_trainset['ensemble'].apply(lambda x: 'stock' if x == 'o' else 'nstock')
        convlog_trainset = convlog_trainset[['content', 'label']]
        X_train, X_val, X_test, y_train, y_val, y_test = data_processor.train_test_split(convlog_trainset, 'content', 'label', \
                                                                                    0.2, 0.1, self.model_config['random_state'])
        train_df = data_processor.data_to_df(list(zip(X_train, y_train)), columns=['text', 'label']).reset_index(drop=True)
        val_df = data_processor.data_to_df(list(zip(X_val, y_val)), columns=['text', 'label']).reset_index(drop=True)
        test_df = data_processor.data_to_df(list(zip(X_test, y_test)), columns=['text', 'label']).reset_index(drop=True)
        print(len(train_df), len(val_df), len(test_df))
        stock_dict = DatasetDict({
            "train": Dataset.from_pandas(train_df), 
            "val": Dataset.from_pandas(val_df),
            "test": Dataset.from_pandas(test_df)
        })
        return stock_dict

    def set_val_tokenizer(self, val_tok_path):
        '''
        í† í° ê°œìˆ˜ ê²€ì‚¬ í† í¬ë‚˜ì´ì €ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤. 
        '''
        return KFDeBERTaTokenizer(val_tok_path)
    
    def set_encoder(self, model_path):
        return KFDeBERTaTokenizer(model_path), KFDeBERTa(model_path)
        
    def initialize_trainer(self, model_path, model_config, dataset):
        '''
        ì¦ê¶Œ ì¢…ëª© ì˜ˆì¸¡ì— ì‚¬ìš©ë˜ëŠ” ì¸ì½”ë” ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤. 
        '''
        tokenizer = KFDeBERTaTokenizer(model_path).tokenizer
        kfdeberta = KFDeBERTa(model_path)
        model = kfdeberta.model
        kfdeberta.set_training_config(model_config)
        training_config = kfdeberta.training_args
        trainer = ModelTrainer(tokenizer, model, training_config)
        trainer.setup_trainer(dataset)
        return trainer

    def initialize_predictor(self, model_path):
        tokenizer = KFDeBERTaTokenizer(model_path).tokenizer
        model = KFDeBERTa(model_path).model
        return ModelPredictor(tokenizer=tokenizer, model=model)


class LLMManager:
    def __init__(self, model_config):
        self.model_config = model_config

    def initialize_openai_llm(self):
        '''
        ChatGPT ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±í•˜ê³  ë°˜í™˜í•©ë‹ˆë‹¤. 
        '''
        openai_llm = LLMOpenAI(self.model_config)
        openai_llm.set_generation_config()
        return openai_llm
        

class PipelineController:
    def __init__(self, env_manager=None, preprocessor=None, db_manager=None, model_manager=None, llm_manager=None):
        self.env_manager = env_manager 
        self.db_manager = db_manager 
        self.preprocessor = preprocessor 
        self.model_manager = model_manager 
        self.llm_manager = llm_manager 
    
    def set_env(self):
        self.tickle_list = self.env_manager.tickle_list 
        self.postgres, self.table_editor = self.db_manager.initialize_database()
        self.data_p, self.text_p, self.vec_p, self.time_p = self.preprocessor.initialize_processor()
        # print(self.model_manager)
        if self.model_manager != None:
            self.val_tokenizer = self.model_manager.set_val_tokenizer(os.path.join(self.env_manager.model_config['model_path'], 'val-tokenizer'))
            self.predictor = self.model_manager.initialize_predictor(os.path.join(self.env_manager.model_config['model_path'], 'kfdeberta', 'model-update'))
            self.openai_llm = self.llm_manager.initialize_openai_llm()

    def process_data(self, input_data):
        '''
        ëŒ€í™” ê¸°ë¡ì„ ë³´ê³ , í•´ë‹¹ ëŒ€í™”ê°€ ì¦ê¶Œ ì¢…ëª© ë¶„ì„ ì§ˆë¬¸ì¸ì§€ ì•„ë‹Œì§€ ë¶„ë¥˜í•œ í›„ PostgreSQL ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥í•©ë‹ˆë‹¤.  
        ì¦ê¶Œ ì¢…ëª© ë¶„ì„ ì§ˆë¬¸ì´ê±°ë‚˜, ì‚¬ìš©ìê°€ ì•± ë‚´ ë²„íŠ¼ì„ í´ë¦­í•´ì„œ ëŒ€í™”ë¥¼ ì‹œì‘í•œ ê²½ìš°, ì¦ê¶Œ ì¢…ëª© ì´ë¦„ì„ ì¶”ì¶œí•©ë‹ˆë‹¤ (í–¥í›„ êµ¬í˜„)
        args:
        input_data (db table): ibk íˆ¬ì ì¦ê¶Œ ì±—ë´‡ì„ ì´ìš©í•œ ì‚¬ìš©ìë“¤ì˜ ëŒ€í™” ë¡œê·¸ [conv_id (pk), date, qa, content, user id]
        
        process:
        Step 1. qa íƒ€ì…ì´ 'a' (ì±—ë´‡ì˜ ì‘ë‹µ) ì´ê±°ë‚˜ ì´ë¯¸ ë°ì´í„°ë² ì´ìŠ¤ì— ì¡´ì¬í•˜ëŠ” ë°ì´í„°ì¸ì§€ ì²´í¬í•œë‹¤. 
        Step 2. ê·¸ ì´ì™¸ì˜ ê²½ìš°, ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•´ encoder ëª¨ë¸, gpt ëª¨ë¸ì„ í™œìš©í•´ ì¦ê¶Œ ì¢…ëª© ì§ˆë¬¸ ì—¬ë¶€ë¥¼ ë¶„ë¥˜í•œë‹¤. 
          Step 2.1. ì„±ëŠ¥ ê°œì„ ì„ ìœ„í•´, ì‚¬ìš©ì ì§ˆë¬¸ì´ ë‹¨ì¼ í† í°ìœ¼ë¡œ ì´ë£¨ì–´ì§„ ê²½ìš°, tickle listì™€ ë§¤í•‘í•´ tickle ê´€ë ¨ ì§ˆë¬¸ì¸ì§€ ì¶”ê°€ ê²€ì‚¬í•œë‹¤.
        Step 3. ì‚¬ìš©ìê°€ ì•± ë‚´ ë²„íŠ¼ì„ í´ë¦­í•œ í›„ ì§ˆë¬¸ì„ í•  ê²½ìš°, (KR: 333333) ê°™ì€ í‘œí˜„ê°’ì´ ëŒ€í™” ê¸°ë¡ì— ë‚¨ëŠ”ë‹¤. ì´ë¥¼ í™œìš©í•´ ì‚¬ìš©ìê°€ 
                ë²„íŠ¼ì„ í´ë¦­í•´ ë“¤ì–´ì˜¨ ì‚¬ìš©ìì¸ì§€ ì•„ë‹Œì§€ ë¶„ë¥˜í•œë‹¤.
        Step 4. ìƒì„±í•œ ë°ì´í„°ì„¸íŠ¸ë¥¼ PostgreSQL ê° í…Œì´ë¸”ì— ì €ì¥í•œë‹¤.
        '''
        for idx in tqdm(range(len(input_data))):    
            if input_data[idx][2] == 'A':
                continue
            if self.postgres.check_pk(self.env_manager.cls_tb_name, input_data[idx][0]):    # ë°ì´í„°ë² ì´ìŠ¤ì— ì´ë¯¸ ì¡´ì¬í•˜ëŠ” ë°ì´í„°ì¸ ê²½ìš°
                continue

            query = input_data[idx][3]
            print(f'query: {query}')
            encoder_response = self.predictor.predict(query)
            enc_res = 'o' if encoder_response == 'stock' else 'x'

            if len(self.val_tokenizer.tokenize_data(query)) == 1:
                cleaned_word = self.text_p.remove_patterns(query, r"(ë‰´ìŠ¤|ì£¼ì‹|ì •ë³´|ë¶„ì„)$")    # ë¶ˆí•„ìš”í•œ ë‹¨ì–´ ì œê±°
                enc_res = 'o' if cleaned_word in self.tickle_list else 'x'
            cls_pred_set = (input_data[idx][0], enc_res)  
            TICKLE_PATTERN = r"\b\w+\(KR:\d+\)"
            clicked = 'o' if self.text_p.check_expr(TICKLE_PATTERN, query) else 'x'
            u_id = input_data[idx][4]
            clicked_set = (input_data[idx][0], clicked, u_id)

            self.table_editor.edit_cls_table('insert', self.env_manager.cls_tb_name, data_type='raw', data=cls_pred_set)
            self.table_editor.edit_clicked_table('insert', self.env_manager.clicked_tb_name, data_type='raw', data=clicked_set)

    def run(self, process='daily', query=None):
        '''
        args:
        '''
        if query:
            self.openai_llm.set_stock_guideline()
            if len(self.val_tokenizer.tokenize_data(query)) == 1:
                cleaned_word = self.text_p.remove_patterns(query, r"(ë‰´ìŠ¤|ì£¼ì‹|ì •ë³´|ë¶„ì„)$")    # ë¶ˆí•„ìš”í•œ ë‹¨ì–´ ì œê±°
                ensembled_res = 'o' if cleaned_word in self.tickle_list else 'x'
                print(f'í•´ë‹¹ ì¿¼ë¦¬ëŠ” ì¢…ëª© ë¶„ì„ {ensembled_res} ì§ˆë¬¸ì…ë‹ˆë‹¤.')
            response = self.openai_llm.get_response(query=query, role=self.openai_llm.system_role, sub_role=self.openai_llm.stock_role)
            print(f'í•´ë‹¹ ì¿¼ë¦¬ëŠ” {response} ì§ˆë¬¸ì…ë‹ˆë‹¤.')
        else:
            yy, mm, dd = self.time_p.get_current_date()
            crawling_date = yy + mm + dd    # 20240201 í˜•ì‹
            input_data = self.postgres.get_total_data(self.env_manager.conv_tb_name) if process == 'code-test' else \
                self.postgres.get_day_data(self.env_manager.conv_tb_name, crawling_date)
            self.process_data(input_data)


class UnifiedPipeline:
    """ë°ì´í„° ìˆ˜ì§‘ê³¼ ë¶„ì„ì„ í†µí•©í•œ íŒŒì´í”„ë¼ì¸"""
    
    def __init__(self, args):
        self.args = args
        self.env_manager = EnvManager(args)
        self.preprocessor = PreProcessor()
        self.db_manager = DBManager(self.env_manager.db_config)
        self.model_manager = ModelManager(self.env_manager.model_config)
        self.llm_manager = LLMManager(self.env_manager.model_config)
        self.api_pipeline = APIPipeline(bearer_tok=self.env_manager.bearer_token)
        
        # í†µí•© íŒŒì´í”„ë¼ì¸ ì»¨íŠ¸ë¡¤ëŸ¬
        self.pipe = PipelineController(
            env_manager=self.env_manager,
            preprocessor=self.preprocessor,
            db_manager=self.db_manager,
            model_manager=self.model_manager,
            llm_manager=self.llm_manager
        )
        self.pipe.set_env()
    
    def collect_data(self):
        """ë°ì´í„° ìˆ˜ì§‘ ë‹¨ê³„"""
        logger = logging.getLogger(__name__)
        logger.info("ğŸš€ ë°ì´í„° ìˆ˜ì§‘ ì‘ì—…ì´ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤.")
        logger.info(f"ğŸ“… ì‹¤í–‰ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
        if self.args.process in ['daily', 'scheduled']:
            # APIë¥¼ í†µí•œ ë°ì´í„° ìˆ˜ì§‘
            current_time = datetime.now()
            start_date = current_time.strftime("%Y-%m-%d")
            logger.info(f"ğŸ“… ë°ì´í„° ìˆ˜ì§‘ ë‚ ì§œ: {start_date}")
            
            # ibkì™€ ibks ë‘ tenant_id ëª¨ë‘ ìˆ˜ì§‘
            all_api_data = []
            tenant_ids = ['ibk', 'ibks']
            for tenant_id in tenant_ids:
                logger.info(f"ğŸ” {tenant_id} tenant ë°ì´í„° ìˆ˜ì§‘ ì¤‘...")
                api_data = self.api_pipeline.get_data(date=start_date, tenant_id=tenant_id)
                if api_data:
                    all_api_data.extend(api_data)
                    logger.info(f"   âœ… {tenant_id}: {len(api_data)}ê°œ ë ˆì½”ë“œ ìˆ˜ì§‘")
                else:
                    logger.info(f"   âš ï¸ {tenant_id}: ë°ì´í„° ì—†ìŒ")
            
            logger.info(f"ğŸ“Š ì´ ìˆ˜ì§‘ëœ API ë°ì´í„°: {len(all_api_data)}ê°œ")
            if not all_api_data:
                logger.warning("âŒ ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return None
            
            input_data = self.api_pipeline.process_data(all_api_data)
            logger.info(f"ì²˜ë¦¬ëœ ë°ì´í„° shape: {input_data.shape}")
            
            if input_data.empty:
                logger.warning("âŒ ì²˜ë¦¬ëœ ë°ì´í„°ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
                return None
            
            return input_data
        
        elif self.args.process == 'code-test':
            # ê¸°ì¡´ íŒŒì¼ì—ì„œ ë°ì´í„° ë¡œë“œ
            logger.info("ğŸ“ ê¸°ì¡´ íŒŒì¼ì—ì„œ ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.")
            if self.args.file_name.split('.')[-1] == 'csv':
                input_data = pd.read_csv(f"{self.args.data_path}/{self.args.file_name}")
            elif self.args.file_name.split('.')[-1] == 'xlsx':
                input_data = pd.read_excel(f"{self.args.data_path}/{self.args.file_name}")
            else:
                logger.error("âŒ ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤.")
                return None
            
            # í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì„ íƒ
            required_columns = ['date', 'q/a', 'content', 'user_id']
            if all(col in input_data.columns for col in required_columns):
                input_data = input_data[required_columns]
            else:
                logger.error(f"âŒ í•„ìš”í•œ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤: {required_columns}")
                return None
            
            return input_data
        
        else:
            logger.error(f"âŒ ì§€ì›í•˜ì§€ ì•ŠëŠ” í”„ë¡œì„¸ìŠ¤ íƒ€ì…ì…ë‹ˆë‹¤: {self.args.process}")
            return None
    
    def process_and_store_data(self, input_data):
        """ë°ì´í„° ì²˜ë¦¬ ë° ì €ì¥ ë‹¨ê³„"""
        logger = logging.getLogger(__name__)
        if input_data is None or input_data.empty:
            logger.warning("âŒ ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return False
        
        logger.info("ğŸ’¾ ë°ì´í„° ì²˜ë¦¬ ë° ì €ì¥ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
        
        # API ë°ì´í„°ì¸ ê²½ìš° ì¶”ê°€ ì»¬ëŸ¼ ì²˜ë¦¬
        if self.args.process in ['daily', 'scheduled']:
            required_columns = ['date', 'q/a', 'content', 'user_id', 'tenant_id', 'hash_value', 'hash_ref']
            missing_columns = [col for col in required_columns if col not in input_data.columns]
            if missing_columns:
                logger.error(f"âŒ í•„ìš”í•œ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤: {missing_columns}")
                return False
            
            input_data = input_data[required_columns]
            
            # ë‚ ì§œë³„ ì¹´ìš´í„° ì´ˆê¸°í™”
            date_counters = {}
            for date_str in input_data['date'].unique():
                date_value = datetime.fromisoformat(date_str)
                kst = timezone(timedelta(hours=9))
                if date_value.tzinfo is None:
                    date_value = date_value.replace(tzinfo=timezone.utc)
                kst_date = date_value.astimezone(kst)
                pk_date = f"{str(kst_date.year)}{str(kst_date.month).zfill(2)}{str(kst_date.day).zfill(2)}"
                
                try:
                    self.pipe.postgres.db_connection.cur.execute(
                        f"SELECT MAX(conv_id) FROM {self.env_manager.conv_tb_name} WHERE conv_id LIKE %s",
                        (f"{pk_date}_%",)
                    )
                    max_conv_id = self.pipe.postgres.db_connection.cur.fetchone()[0]
                    date_counters[pk_date] = int(max_conv_id.split('_')[1]) if max_conv_id else 0
                except:
                    date_counters[pk_date] = 0
            
            # conv_id ìƒì„± ë° KST ë³€í™˜
            conv_ids = []
            for idx in tqdm(range(len(input_data))):
                date_value = datetime.fromisoformat(input_data['date'][idx])
                kst = timezone(timedelta(hours=9))
                if date_value.tzinfo is None:
                    date_value = date_value.replace(tzinfo=timezone.utc)
                kst_date = date_value.astimezone(kst)
                
                input_data.at[idx, 'date'] = kst_date.isoformat()
                pk_date = f"{str(kst_date.year)}{str(kst_date.month).zfill(2)}{str(kst_date.day).zfill(2)}"
                date_counters[pk_date] += 1
                conv_ids.append(f"{pk_date}_{str(date_counters[pk_date]).zfill(5)}")
            
            input_data.insert(0, 'conv_id', conv_ids)
            input_data = input_data[['conv_id', 'date', 'q/a', 'content', 'user_id', 'tenant_id', 'hash_value', 'hash_ref']]
            
        else:
            # ê¸°ì¡´ íŒŒì¼ ë°ì´í„° ì²˜ë¦¬
            conv_ids = []
            for idx in tqdm(range(len(input_data))):
                date_value = input_data['date'][idx]
                pk_date = f"{str(date_value.year)}{str(date_value.month).zfill(2)}{str(date_value.day).zfill(2)}"
                conv_id = pk_date + '_' + str(idx).zfill(5)
                conv_ids.append(conv_id)
            input_data.insert(0, 'conv_id', conv_ids)
        
        # í†µê³„ ì¶œë ¥
        if 'q/a' in input_data.columns:
            q_count = sum(1 for qa in input_data['q/a'] if qa == 'Q')
            a_count = sum(1 for qa in input_data['q/a'] if qa == 'A')
            logger.info(f"ğŸ“Š Q&A í†µê³„: Q {q_count}ê°œ, A {a_count}ê°œ")
            
            if 'hash_ref' in input_data.columns:
                a_with_ref = sum(1 for ref in input_data['hash_ref'] if ref is not None)
                logger.info(f"ğŸ“Š Aì— hash_ref ìˆìŒ: {a_with_ref}ê°œ")
        
        # ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
        total_records = len(input_data)
        existing_records = 0
        new_records = 0
        
        for idx in tqdm(range(len(input_data))):
            # ì¤‘ë³µ ì²´í¬ (API ë°ì´í„°ì¸ ê²½ìš° í•´ì‹œê°’ìœ¼ë¡œ, íŒŒì¼ ë°ì´í„°ì¸ ê²½ìš° PKë¡œ)
            if self.args.process in ['daily', 'scheduled'] and 'hash_value' in input_data.columns:
                if self.pipe.postgres.check_hash_duplicate(self.env_manager.conv_tb_name, input_data['hash_value'][idx]):
                    existing_records += 1
                    logger.info(f"ì´ë¯¸ ì¡´ì¬í•˜ëŠ” ë°ì´í„° (í•´ì‹œ: {input_data['hash_value'][idx][:8]}...): {input_data['conv_id'][idx]}")
                    continue
            else:
                if self.pipe.postgres.check_pk(self.env_manager.conv_tb_name, input_data['conv_id'][idx]):
                    existing_records += 1
                    logger.info(f"ì´ë¯¸ ì¡´ì¬í•˜ëŠ” ë°ì´í„°: {input_data['conv_id'][idx]}")
                    continue
            
            new_records += 1
            data_set = tuple(input_data.iloc[idx].values)
            self.pipe.table_editor.edit_conv_table('insert', self.env_manager.conv_tb_name, data_type='raw', data=data_set)
        
        # ì €ì¥ ê²°ê³¼ ìš”ì•½
        summary_msg = f"ğŸ“Š ë°ì´í„° ì €ì¥ ì™„ë£Œ - ì „ì²´: {total_records}, ì‹ ê·œ: {new_records}, ì¤‘ë³µ: {existing_records}"
        logger.info(summary_msg)
        logger.info(f"   ì¤‘ë³µë¥ : {(existing_records/total_records*100):.1f}%" if total_records > 0 else "   ì¤‘ë³µë¥ : 0%")
        
        return True
    
    def run_analysis(self):
        """ë¶„ì„ ë‹¨ê³„ ì‹¤í–‰"""
        logger = logging.getLogger(__name__)
        logger.info("ğŸ” ë°ì´í„° ë¶„ì„ ì‘ì—…ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
        
        try:
            # ë¶„ì„ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰
            self.pipe.run(process=self.args.process, query=self.args.query)
            logger.info("âœ… ë°ì´í„° ë¶„ì„ ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
            return True
        except Exception as e:
            logger.error(f"âŒ ë°ì´í„° ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            return False
    
    def run_full_pipeline(self):
        """ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        logger = logging.getLogger(__name__)
        logger.info("=== í†µí•© íŒŒì´í”„ë¼ì¸ ì‹œì‘ ===")
        
        try:
            # 1ë‹¨ê³„: ë°ì´í„° ìˆ˜ì§‘
            input_data = self.collect_data()
            if input_data is None:
                logger.warning("âš ï¸ ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨ë¡œ íŒŒì´í”„ë¼ì¸ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                return False
            
            # 2ë‹¨ê³„: ë°ì´í„° ì²˜ë¦¬ ë° ì €ì¥
            if not self.process_and_store_data(input_data):
                logger.warning("âš ï¸ ë°ì´í„° ì €ì¥ ì‹¤íŒ¨ë¡œ íŒŒì´í”„ë¼ì¸ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                return False
            
            # 3ë‹¨ê³„: ë°ì´í„° ë¶„ì„ (main.pyì˜ ê¸°ëŠ¥)
            if not self.run_analysis():
                logger.warning("âš ï¸ ë°ì´í„° ë¶„ì„ ì‹¤íŒ¨")
                return False
            
            logger.info("=== í†µí•© íŒŒì´í”„ë¼ì¸ ì™„ë£Œ ===")
            return True
            
        except Exception as e:
            logger.error(f"âŒ í†µí•© íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            return False
        finally:
            # ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì¢…ë£Œ
            if hasattr(self.pipe, 'postgres') and hasattr(self.pipe.postgres, 'db_connection'):
                self.pipe.postgres.db_connection.close()
