from src import EnvManager, PreProcessor, DBManager, APIPipeline, PipelineController
from tqdm import tqdm
import pandas as pd
from dotenv import load_dotenv
import argparse 
import logging
import time
import os
from datetime import datetime, timezone, timedelta
from apscheduler.schedulers.background import BackgroundScheduler
from apscheduler.triggers.cron import CronTrigger
from scheduler_config import get_schedule_config, print_available_schedules

logging.basicConfig(
    level=logging.INFO,  # ë¡œê·¸ ë ˆë²¨ ì„¤ì • (DEBUG, INFO, WARNING, ERROR, CRITICAL)
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("app.log"),# mode='w'),  # ë¡œê·¸ë¥¼ íŒŒì¼ì— ê¸°ë¡
    ]
)
logging.basicConfig(filename='warnings.log', level=logging.WARNING)
logging.captureWarnings(True)

def main(args):
    logger = logging.getLogger(__name__)
    env_manager = EnvManager(args)
    preprocessor = PreProcessor()
    db_manager = DBManager(env_manager.db_config)
    api_pipeline = APIPipeline(bearer_tok=env_manager.bearer_token) 

    pipe = PipelineController(env_manager=env_manager, preprocessor=preprocessor, db_manager=db_manager)   
    pipe.set_env()

    if args.process == 'code-test':   # ì €ì¥í•  íŒŒì¼ëª… ì§€ì •   
        if args.file_name.split('.')[-1] == 'csv': 
            input_data = pd.read_csv(os.path.join(args.data_path, args.file_name))
        elif args.file_name.split('.')[-1] == 'xlsx':
            input_data = pd.read_excel(os.path.join(args.data_path, args.file_name))
    elif args.process == 'daily':    # ë‚ ì§œ ë²”ìœ„ ì§€ì •í•˜ì—¬ ë°ì´í„° ì €ì¥
        # API í˜¸ì¶œìš© ë‚ ì§œ ì„¤ì • (KST ë³€í™˜ ì—†ì´ ê·¸ëŒ€ë¡œ ì‚¬ìš©)
        from_date = "2025-09-22"
        to_date = "2025-09-23"
        print(f"ğŸ“… API ìš”ì²­ ë‚ ì§œ: {from_date} ~ {to_date}")
        
        # ë‚ ì§œ ë²”ìœ„ì— ëŒ€í•´ API í˜¸ì¶œ (ibk, ibks ëª¨ë‘ ìˆ˜ì§‘)
        all_api_data = []
        tenant_ids = ['ibk', 'ibks']
        
        current_date = datetime.strptime(from_date, "%Y-%m-%d")
        end_date_obj = datetime.strptime(to_date, "%Y-%m-%d")
        
        while current_date <= end_date_obj:
            date_str = current_date.strftime("%Y-%m-%d")
            print(f"ğŸ” {date_str} ë°ì´í„° ìˆ˜ì§‘ ì¤‘...")
            
            for tenant_id in tenant_ids:
                print(f"   ğŸ“‹ {tenant_id} tenant ìˆ˜ì§‘ ì¤‘...")
                api_data = api_pipeline.get_data(date=date_str, tenant_id=tenant_id)
                if api_data:
                    all_api_data.extend(api_data)
                    print(f"      âœ… {tenant_id}: {len(api_data)}ê°œ ë ˆì½”ë“œ ìˆ˜ì§‘")
                else:
                    print(f"      âš ï¸ {tenant_id}: ë°ì´í„° ì—†ìŒ")
            
            current_date += timedelta(days=1)
        
        print(f"ğŸ“Š ì´ ìˆ˜ì§‘ëœ API ë°ì´í„°: {len(all_api_data)}ê°œ")
        
        if not all_api_data:
            print("âŒ ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        input_data = api_pipeline.process_data(all_api_data)
        print(f"ì²˜ë¦¬ëœ ë°ì´í„° shape: {input_data.shape}")        
        if input_data.empty:
            print("âŒ ì²˜ë¦¬ëœ ë°ì´í„°ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
            return
        else:
            print(input_data.head())

    input_data = input_data[['date', 'q/a', 'content', 'user_id', 'tenant_id', 'hash_value', 'hash_ref']]
    
    # ë‚ ì§œë³„ conv_id ì¹´ìš´í„° ì´ˆê¸°í™”
    date_counters = {}
    
    # ê¸°ì¡´ DBì—ì„œ ê° ë‚ ì§œë³„ ìµœëŒ€ conv_id ì¡°íšŒ
    for date_str in input_data['date'].unique():
        date_value = datetime.fromisoformat(date_str)
        kst = timezone(timedelta(hours=9))
        if date_value.tzinfo is None:
            date_value = date_value.replace(tzinfo=timezone.utc)
        kst_date = date_value.astimezone(kst)
        pk_date = f"{str(kst_date.year)}{str(kst_date.month).zfill(2)}{str(kst_date.day).zfill(2)}"
        
        try:
            pipe.postgres.db_connection.cur.execute(
                f"SELECT MAX(conv_id) FROM {pipe.env_manager.conv_tb_name} WHERE conv_id LIKE %s",
                (f"{pk_date}_%",)
            )
            max_conv_id = pipe.postgres.db_connection.cur.fetchone()[0]
            date_counters[pk_date] = int(max_conv_id.split('_')[1]) if max_conv_id else 0
        except:
            date_counters[pk_date] = 0
    
    # conv_id ìƒì„± ë° KST ë³€í™˜
    conv_ids = []
    for idx in tqdm(range(len(input_data))):
        date_value = datetime.fromisoformat(input_data['date'][idx])
        kst = timezone(timedelta(hours=9))
        if date_value.tzinfo is None:
            date_value = date_value.replace(tzinfo=timezone.utc)
        kst_date = date_value.astimezone(kst)
        
        input_data.at[idx, 'date'] = kst_date.isoformat()
        pk_date = f"{str(kst_date.year)}{str(kst_date.month).zfill(2)}{str(kst_date.day).zfill(2)}"
        date_counters[pk_date] += 1
        conv_ids.append(f"{pk_date}_{str(date_counters[pk_date]).zfill(5)}")
    
    input_data.insert(0, 'conv_id', conv_ids)
    
    # Q&A ì—°ê²° í†µê³„
    q_count = sum(1 for qa in input_data['q/a'] if qa == 'Q')
    a_count = sum(1 for qa in input_data['q/a'] if qa == 'A')
    a_with_ref = sum(1 for ref in input_data['hash_ref'] if ref is not None)
    print(f"ğŸ“Š Q&A ì—°ê²° í†µê³„: Q {q_count}ê°œ, A {a_count}ê°œ, Aì— hash_ref ìˆìŒ {a_with_ref}ê°œ")
    
    # ì¤‘ë³µ ì €ì¥ ë°©ì§€ í†µê³„
    total_records = len(input_data)
    existing_records = 0
    new_records = 0
    
    for idx in tqdm(range(len(input_data))):   # PostgreSQL í…Œì´ë¸”ì— ë°ì´í„° ì €ì¥
        # í•´ì‹œê°’ ê¸°ì¤€ìœ¼ë¡œ ì¤‘ë³µ ì²´í¬
        if pipe.postgres.check_hash_duplicate(pipe.env_manager.conv_tb_name, input_data['hash_value'][idx]):
            existing_records += 1
            logger.info(f"ì´ë¯¸ ì¡´ì¬í•˜ëŠ” ë°ì´í„° (í•´ì‹œ: {input_data['hash_value'][idx][:8]}...): {input_data['conv_id'][idx]}")
            continue
        
        new_records += 1
        data_set = tuple(input_data.iloc[idx].values)
        
        # ë””ë²„ê¹…: ì €ì¥í•  ë°ì´í„° í™•ì¸ (ì²˜ìŒ 3ê°œë§Œ)
        if idx < 3:
            print(f"ğŸ” ì €ì¥í•  ë°ì´í„° {idx}: {data_set}")
            print(f"   - conv_id: {data_set[0]}")
            print(f"   - hash_value: {data_set[1]}")
            print(f"   - hash_ref: {data_set[2]}")
            print(f"   - q/a: {data_set[4]}")
        
        pipe.table_editor.edit_conv_table('insert', pipe.env_manager.conv_tb_name, data_type='raw', data=data_set)
    
    # ì €ì¥ ê²°ê³¼ ìš”ì•½
    print(f"\nğŸ“Š ë°ì´í„° ì €ì¥ ê²°ê³¼:")
    print(f"   ì „ì²´ ë ˆì½”ë“œ: {total_records}")
    print(f"   ìƒˆë¡œ ì €ì¥ëœ ë ˆì½”ë“œ: {new_records}")
    print(f"   ì´ë¯¸ ì¡´ì¬í•˜ëŠ” ë ˆì½”ë“œ: {existing_records}")
    print(f"   ì¤‘ë³µë¥ : {(existing_records/total_records*100):.1f}%" if total_records > 0 else "   ì¤‘ë³µë¥ : 0%")            
    pipe.postgres.db_connection.close()

if __name__ == '__main__':
    cli_parser = argparse.ArgumentParser()
    cli_parser.add_argument('--data_path', type=str, default='./data/')
    cli_parser.add_argument('--file_name', type=str, default='conv_log-0705-0902.xlsx')
    cli_parser.add_argument('--config_path', type=str, default='./config/')
    cli_parser.add_argument('--task_name', type=str, default='cls')
    cli_parser.add_argument('--process', type=str, default='daily', 
                           help='ì‹¤í–‰ ëª¨ë“œ: daily(ì¼íšŒì„±), scheduled(ìŠ¤ì¼€ì¤„ë§)')
    cli_parser.add_argument('--schedule_type', type=str, default='hourly',
                           help='ìŠ¤ì¼€ì¤„ íƒ€ì…: hourly, daily, every_30min, every_15min, business_hours')
    cli_parser.add_argument('--query', type=str, default=None)
    cli_args = cli_parser.parse_args()
    
    # ìŠ¤ì¼€ì¤„ë§ ëª¨ë“œ í™•ì¸
    if cli_args.process == 'scheduled':
        # ì‚¬ìš© ê°€ëŠ¥í•œ ìŠ¤ì¼€ì¤„ ì˜µì…˜ ì¶œë ¥
        print_available_schedules()
        
        # ìŠ¤ì¼€ì¤„ ì„¤ì • ê°€ì ¸ì˜¤ê¸°
        schedule_config = get_schedule_config(cli_args.schedule_type)
        print(f"\nâœ… ì„ íƒëœ ìŠ¤ì¼€ì¤„: {schedule_config['description']}")
        
        # APSchedulerë¥¼ ì‚¬ìš©í•œ ìŠ¤ì¼€ì¤„ë§
        scheduler = BackgroundScheduler()
        
        # ìŠ¤ì¼€ì¤„ëœ ì‘ì—… ì¶”ê°€
        scheduler.add_job(
            func=main,
            trigger=schedule_config['trigger'],
            args=[cli_args],
            id='data_collection_job',
            name=f'ë°ì´í„° ìˆ˜ì§‘ ({cli_args.schedule_type})',
            replace_existing=True,
            max_instances=1  # ë™ì‹œ ì‹¤í–‰ ë°©ì§€
        )
        
        # ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘
        scheduler.start()
        logger = logging.getLogger(__name__)
        logger.info(f"ğŸ• APSchedulerê°€ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤. {schedule_config['description']}")
        logger.info("ğŸ“… ì˜ˆì •ëœ ì‘ì—…ë“¤:")
        for job in scheduler.get_jobs():
            logger.info(f"   - {job.name}: {job.next_run_time}")
        
        try:
            # ë©”ì¸ ìŠ¤ë ˆë“œ ìœ ì§€
            while True:
                time.sleep(60)  # 1ë¶„ë§ˆë‹¤ ì²´í¬
        except KeyboardInterrupt:
            logger.info("â¹ï¸ ìŠ¤ì¼€ì¤„ëŸ¬ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤...")
            scheduler.shutdown()
    else:
        # ì¼íšŒì„± ì‹¤í–‰
        main(cli_args)