from src import EnvManager, PreProcessor, DBManager, APIPipeline, PipelineController
from tqdm import tqdm
import pandas as pd
from dotenv import load_dotenv
import argparse 
import logging
import schedule
import time
import os
from datetime import datetime

logging.basicConfig(
    level=logging.INFO,  # ë¡œê·¸ ë ˆë²¨ ì„¤ì • (DEBUG, INFO, WARNING, ERROR, CRITICAL)
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("app.log"),# mode='w'),  # ë¡œê·¸ë¥¼ íŒŒì¼ì— ê¸°ë¡
    ]
)
logging.basicConfig(filename='warnings.log', level=logging.WARNING)
logging.captureWarnings(True)

def main(args):
    logger = logging.getLogger(__name__)
    env_manager = EnvManager(args)
    preprocessor = PreProcessor()
    db_manager = DBManager(env_manager.db_config)
    api_pipeline = APIPipeline(bearer_tok=env_manager.bearer_token) 

    pipe = PipelineController(env_manager=env_manager, preprocessor=preprocessor, db_manager=db_manager)   
    pipe.set_env()

    if args.process == 'code-test':   # ì €ì¥í•  íŒŒì¼ëª… ì§€ì •   
        if args.file_name.split('.')[-1] == 'csv': 
            input_data = pd.read_csv(os.path.join(args.data_path, args.file_name))
        elif args.file_name.split('.')[-1] == 'xlsx':
            input_data = pd.read_excel(os.path.join(args.data_path, args.file_name))
    elif args.process == 'daily':    # ë§¤ì¼ 12ì‹œ 10ë¶„ì— ì „ì¼ ë°ì´í„° ì €ì¥
        yy, mm, dd = pipe.time_p.get_previous_day_date()
        start_date = yy + "-" + mm + "-" + dd
        api_data = api_pipeline.get_data(date=start_date, tenant_id='ibk')        
        if api_data:
            print(f"ì²« ë²ˆì§¸ ë°ì´í„° ìƒ˜í”Œ: {api_data[0] if api_data else 'None'}")
        
        input_data = api_pipeline.process_data(api_data)
        print(f"ì²˜ë¦¬ëœ ë°ì´í„° shape: {input_data.shape}")        
        if input_data.empty:
            print("âŒ ì²˜ë¦¬ëœ ë°ì´í„°ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ë‚ ì§œë¥¼ ì‹œë„í•´ë³´ì„¸ìš”.")
            return
        else:
            print(input_data.head())

    input_data = input_data[['date', 'q/a', 'content', 'user_id', 'tenant_id']]
    conv_ids = []
    for idx in tqdm(range(len(input_data))):   # ì±—ë´‡ ëŒ€í™” ë¡œê·¸ ë°ì´í„°ì— PK ì¶”ê°€ 
        date_str = input_data['date'][idx]
        # ë‚ ì§œ ë¬¸ìì—´ì„ datetime ê°ì²´ë¡œ ë³€í™˜
        if isinstance(date_str, str):
            date_value = datetime.fromisoformat(date_str)
        else:
            date_value = date_str
        pk_date = f"{str(date_value.year)}{str(date_value.month).zfill(2)}{str(date_value.day).zfill(2)}"
        
        # ë” ì•ˆì „í•œ conv_id ìƒì„±: ë‚ ì§œ + ì‚¬ìš©ìID + ë‚´ìš© í•´ì‹œ
        import hashlib
        content_hash = hashlib.md5(
            f"{input_data['user_id'][idx]}_{input_data['content'][idx]}_{input_data['q/a'][idx]}".encode()
        ).hexdigest()[:8]
        conv_id = f"{pk_date}_{input_data['user_id'][idx]}_{content_hash}"
        conv_ids.append(conv_id)
    input_data.insert(0, 'conv_id', conv_ids)
    
    # ì¤‘ë³µ ì €ì¥ ë°©ì§€ í†µê³„
    total_records = len(input_data)
    existing_records = 0
    new_records = 0
    
    for idx in tqdm(range(len(input_data))):   # PostgreSQL í…Œì´ë¸”ì— ë°ì´í„° ì €ì¥
        if pipe.postgres.check_pk(pipe.env_manager.conv_tb_name, input_data['conv_id'][idx]):   # ë°ì´í„° ì¡´ì¬ ì—¬ë¶€ í™•ì¸
            existing_records += 1
            logger.info(f"ì´ë¯¸ ì¡´ì¬í•˜ëŠ” ë°ì´í„°: {input_data['conv_id'][idx]}")
            continue
        
        new_records += 1
        data_set = tuple(input_data.iloc[idx].values)
        pipe.table_editor.edit_conv_table('insert', pipe.env_manager.conv_tb_name, data_type='raw', data=data_set)
    
    # ì €ì¥ ê²°ê³¼ ìš”ì•½
    print(f"\nğŸ“Š ë°ì´í„° ì €ì¥ ê²°ê³¼:")
    print(f"   ì „ì²´ ë ˆì½”ë“œ: {total_records}")
    print(f"   ìƒˆë¡œ ì €ì¥ëœ ë ˆì½”ë“œ: {new_records}")
    print(f"   ì´ë¯¸ ì¡´ì¬í•˜ëŠ” ë ˆì½”ë“œ: {existing_records}")
    print(f"   ì¤‘ë³µë¥ : {(existing_records/total_records*100):.1f}%" if total_records > 0 else "   ì¤‘ë³µë¥ : 0%")            
    pipe.postgres.db_connection.close()

if __name__ == '__main__':
    cli_parser = argparse.ArgumentParser()
    cli_parser.add_argument('--data_path', type=str, default='./data/')
    cli_parser.add_argument('--file_name', type=str, default='conv_log-0705-0902.xlsx')
    cli_parser.add_argument('--config_path', type=str, default='./config/')
    cli_parser.add_argument('--task_name', type=str, default='cls')
    cli_parser.add_argument('--process', type=str, default='daily')
    cli_parser.add_argument('--query', type=str, default=None)
    cli_args = cli_parser.parse_args()
    
    '''schedule.every().day.at("05:10").do(main, cli_args)
    while True:   # ìŠ¤ì¼€ì¤„ëŸ¬ë¥¼ ìœ ì§€í•˜ëŠ” ë£¨í”„
        schedule.run_pending()   # ëŒ€ê¸° ì¤‘ì¸ ì‘ì—… ì‹¤í–‰
        time.sleep(1)'''
    main(cli_args)