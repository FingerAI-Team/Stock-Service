from src import EnvManager, PreProcessor, DBManager, APIPipeline, PipelineController
from tqdm import tqdm
import pandas as pd
from dotenv import load_dotenv
import argparse 
import logging
import time
import os
from datetime import datetime
from apscheduler.schedulers.background import BackgroundScheduler
from apscheduler.triggers.cron import CronTrigger
from scheduler_config import get_schedule_config, print_available_schedules

# ë¡œê¹… ì„¤ì • - íŒŒì¼ í•¸ë“¤ëŸ¬ì™€ ì½˜ì†” í•¸ë“¤ëŸ¬ ëª¨ë‘ ì„¤ì •
logger = logging.getLogger()
logger.setLevel(logging.INFO)

# íŒŒì¼ í•¸ë“¤ëŸ¬ (ëª¨ë“  ë¡œê·¸)
file_handler = logging.FileHandler("app.log", encoding='utf-8')
file_handler.setLevel(logging.INFO)
file_formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
file_handler.setFormatter(file_formatter)

# ê²½ê³  ì „ìš© íŒŒì¼ í•¸ë“¤ëŸ¬
warning_handler = logging.FileHandler("warnings.log", encoding='utf-8')
warning_handler.setLevel(logging.WARNING)
warning_formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
warning_handler.setFormatter(warning_formatter)

# ì½˜ì†” í•¸ë“¤ëŸ¬ (ìŠ¤ì¼€ì¤„ëŸ¬ ì‹¤í–‰ ì‹œ í™•ì¸ìš©)
console_handler = logging.StreamHandler()
console_handler.setLevel(logging.INFO)
console_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
console_handler.setFormatter(console_formatter)

# í•¸ë“¤ëŸ¬ ì¶”ê°€
logger.addHandler(file_handler)
logger.addHandler(warning_handler)
logger.addHandler(console_handler)

logging.captureWarnings(True)

def main(args):
    logger = logging.getLogger(__name__)
    logger.info("ğŸš€ ë°ì´í„° ìˆ˜ì§‘ ì‘ì—…ì´ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤.")
    logger.info(f"ğŸ“… ì‹¤í–‰ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    env_manager = EnvManager(args)
    preprocessor = PreProcessor()
    db_manager = DBManager(env_manager.db_config)
    api_pipeline = APIPipeline(bearer_tok=env_manager.bearer_token) 

    pipe = PipelineController(env_manager=env_manager, preprocessor=preprocessor, db_manager=db_manager)   
    pipe.set_env()

    # input_data ì´ˆê¸°í™”
    input_data = None

    if args.process == 'code-test':   # ì €ì¥í•  íŒŒì¼ëª… ì§€ì •   
        file_extension = args.file_name.split('.')[-1].lower()
        file_path = os.path.join(args.data_path, args.file_name)
        
        if file_extension == 'csv': 
            input_data = pd.read_csv(file_path)
        elif file_extension == 'xlsx':
            input_data = pd.read_excel(file_path)
        else:
            logger.error(f"âŒ ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤: {file_extension}")
            logger.error("ì§€ì› í˜•ì‹: csv, xlsx")
            return
    elif args.process == 'daily':    # ë§¤ì¼ 12ì‹œ 10ë¶„ì— ë‹¹ì¼ ë°ì´í„° ì €ì¥
        # ë‹¹ì¼ ë‚ ì§œ ê¸°ì¤€ìœ¼ë¡œ API í˜¸ì¶œ (ibk, ibks ëª¨ë‘ ìˆ˜ì§‘)
        from datetime import datetime
        today = datetime.now().strftime("%Y-%m-%d")
        logger.info(f"ğŸ“… ë‹¹ì¼ ë°ì´í„° ìˆ˜ì§‘: {today}")
        
        # ibkì™€ ibks ë‘ tenant_id ëª¨ë‘ ìˆ˜ì§‘
        all_api_data = []
        tenant_ids = ['ibk', 'ibks']
        
        for tenant_id in tenant_ids:
            logger.info(f"ğŸ” {tenant_id} tenant ë°ì´í„° ìˆ˜ì§‘ ì¤‘...")
            api_data = api_pipeline.get_data(date=today, tenant_id=tenant_id)
            if api_data:
                all_api_data.extend(api_data)
                logger.info(f"   âœ… {tenant_id}: {len(api_data)}ê°œ ë ˆì½”ë“œ ìˆ˜ì§‘")
            else:
                logger.info(f"   âš ï¸ {tenant_id}: ë°ì´í„° ì—†ìŒ")
        
        logger.info(f"ğŸ“Š ì´ ìˆ˜ì§‘ëœ API ë°ì´í„°: {len(all_api_data)}ê°œ")
        
        if not all_api_data:
            logger.warning("âŒ ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        input_data = api_pipeline.process_data(all_api_data)
        print(f"ì²˜ë¦¬ëœ ë°ì´í„° shape: {input_data.shape}")        
        if input_data.empty:
            logger.warning("âŒ ì²˜ë¦¬ëœ ë°ì´í„°ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
            return
        else:
            print(input_data.head())
    elif args.process == 'scheduled':  # ìŠ¤ì¼€ì¤„ë§ ëª¨ë“œ - ë§¤ì‹œê°„ ì‹¤í–‰
        # í˜„ì¬ ì‹œê°„ ê¸°ì¤€ìœ¼ë¡œ ë°ì´í„° ìˆ˜ì§‘ (ibk, ibks ëª¨ë‘ ìˆ˜ì§‘)
        current_time = datetime.now()
        # ë§¤ì‹œê°„ ì‹¤í–‰ì´ë¯€ë¡œ í˜„ì¬ ì‹œê°„ì˜ ë°ì´í„°ë¥¼ ìˆ˜ì§‘
        start_date = current_time.strftime("%Y-%m-%d")
        logger.info(f"ğŸ“… ìŠ¤ì¼€ì¤„ë§ ëª¨ë“œ - í˜„ì¬ ì‹œê°„ ë°ì´í„° ìˆ˜ì§‘: {start_date}")
        
        # ibkì™€ ibks ë‘ tenant_id ëª¨ë‘ ìˆ˜ì§‘
        all_api_data = []
        tenant_ids = ['ibk', 'ibks']
        
        for tenant_id in tenant_ids:
            logger.info(f"ğŸ” {tenant_id} tenant ë°ì´í„° ìˆ˜ì§‘ ì¤‘...")
            api_data = api_pipeline.get_data(date=start_date, tenant_id=tenant_id)
            if api_data:
                all_api_data.extend(api_data)
                logger.info(f"   âœ… {tenant_id}: {len(api_data)}ê°œ ë ˆì½”ë“œ ìˆ˜ì§‘")
            else:
                logger.info(f"   âš ï¸ {tenant_id}: ë°ì´í„° ì—†ìŒ")
        
        logger.info(f"ğŸ“Š ì´ ìˆ˜ì§‘ëœ API ë°ì´í„°: {len(all_api_data)}ê°œ")
        
        if not all_api_data:
            logger.warning("âŒ ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        input_data = api_pipeline.process_data(all_api_data)
        print(f"ì²˜ë¦¬ëœ ë°ì´í„° shape: {input_data.shape}")        
        if input_data.empty:
            logger.warning("âŒ ì²˜ë¦¬ëœ ë°ì´í„°ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. ë°ì´í„°ê°€ ì—†ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            return
        else:
            print(input_data.head())
    else:
        logger.error(f"âŒ ì§€ì›í•˜ì§€ ì•ŠëŠ” í”„ë¡œì„¸ìŠ¤ íƒ€ì…ì…ë‹ˆë‹¤: {args.process}")
        logger.error("ì§€ì› íƒ€ì…: code-test, daily, scheduled")
        return

    # input_dataê°€ Noneì´ê±°ë‚˜ ë¹„ì–´ìˆëŠ” ê²½ìš° ì²´í¬
    if input_data is None:
        logger.error("âŒ input_dataê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return
        
    if input_data.empty:
        logger.warning("âŒ input_dataê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
        return

    # í•„ìš”í•œ ì»¬ëŸ¼ì´ ìˆëŠ”ì§€ í™•ì¸
    required_columns = ['date', 'q/a', 'content', 'user_id', 'tenant_id']
    missing_columns = [col for col in required_columns if col not in input_data.columns]
    if missing_columns:
        logger.error(f"âŒ í•„ìš”í•œ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤: {missing_columns}")
        logger.error(f"í˜„ì¬ ì»¬ëŸ¼: {list(input_data.columns)}")
        return

    input_data = input_data[required_columns]
    conv_ids = []
    content_hashes = []
    hash_refs = []
    
    # Q&A ìŒì„ ìœ„í•œ ì„ì‹œ ì €ì¥ì†Œ
    qa_pairs = {}  # {qa_key: q_hash_value}
    
    # ë‚ ì§œë³„ ì¸ë±ìŠ¤ ì¹´ìš´í„°ë¥¼ ìœ„í•œ ë”•ì…”ë„ˆë¦¬ (ê¸°ì¡´ DBì˜ ìµœëŒ€ê°’ë¶€í„° ì‹œì‘)
    date_counters = {}
    
    # ê¸°ì¡´ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ê° ë‚ ì§œë³„ ìµœëŒ€ conv_id ë²ˆí˜¸ ì¡°íšŒ
    print("ğŸ” ê¸°ì¡´ ë°ì´í„°ë² ì´ìŠ¤ì˜ conv_id ë²”ìœ„ í™•ì¸ ì¤‘...")
    for idx in range(len(input_data)):
        date_str = input_data['date'][idx]
        if isinstance(date_str, str):
            date_value = datetime.fromisoformat(date_str)
        else:
            date_value = date_str
        
        # UTCë¥¼ ì„œìš¸ ì‹œê°„(KST, UTC+9)ìœ¼ë¡œ ë³€í™˜
        from datetime import timezone, timedelta
        kst = timezone(timedelta(hours=9))
        if date_value.tzinfo is None:
            # timezone ì •ë³´ê°€ ì—†ìœ¼ë©´ UTCë¡œ ê°€ì •
            date_value = date_value.replace(tzinfo=timezone.utc)
        kst_date = date_value.astimezone(kst)
        
        pk_date = f"{str(kst_date.year)}{str(kst_date.month).zfill(2)}{str(kst_date.day).zfill(2)}"
        
        if pk_date not in date_counters:
            # í•´ë‹¹ ë‚ ì§œì˜ ê¸°ì¡´ ìµœëŒ€ conv_id ë²ˆí˜¸ ì¡°íšŒ
            try:
                pipe.postgres.db_connection.cur.execute(
                    f"SELECT MAX(CAST(SUBSTRING(conv_id FROM 10) AS INTEGER)) FROM {pipe.env_manager.conv_tb_name} WHERE conv_id LIKE %s",
                    (f"{pk_date}_%",)
                )
                result = pipe.postgres.db_connection.cur.fetchone()
                max_existing = result[0] if result[0] is not None else -1
                date_counters[pk_date] = max_existing
                print(f"   {pk_date}: ê¸°ì¡´ ìµœëŒ€ ë²ˆí˜¸ {max_existing}, ë‹¤ìŒ ë²ˆí˜¸ë¶€í„° ì‹œì‘")
            except Exception as e:
                print(f"   {pk_date}: ê¸°ì¡´ ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨, 0ë¶€í„° ì‹œì‘ ({e})")
                date_counters[pk_date] = -1
    
    for idx in tqdm(range(len(input_data))):   # ì±—ë´‡ ëŒ€í™” ë¡œê·¸ ë°ì´í„°ì— PK ì¶”ê°€ 
        date_str = input_data['date'][idx]
        # ë‚ ì§œ ë¬¸ìì—´ì„ datetime ê°ì²´ë¡œ ë³€í™˜
        if isinstance(date_str, str):
            date_value = datetime.fromisoformat(date_str)
        else:
            date_value = date_str
        
        # UTCë¥¼ ì„œìš¸ ì‹œê°„(KST, UTC+9)ìœ¼ë¡œ ë³€í™˜
        from datetime import timezone, timedelta
        kst = timezone(timedelta(hours=9))
        if date_value.tzinfo is None:
            # timezone ì •ë³´ê°€ ì—†ìœ¼ë©´ UTCë¡œ ê°€ì •
            date_value = date_value.replace(tzinfo=timezone.utc)
        kst_date = date_value.astimezone(kst)
        
        # date ì»¬ëŸ¼ì— ì €ì¥í•  ê°’ë„ KSTë¡œ ë³€í™˜
        input_data.at[idx, 'date'] = kst_date.isoformat()
        
        pk_date = f"{str(kst_date.year)}{str(kst_date.month).zfill(2)}{str(kst_date.day).zfill(2)}"
        
        # ë‚ ì§œë³„ë¡œ ê³ ìœ í•œ ì¸ë±ìŠ¤ ìƒì„± (ê¸°ì¡´ ìµœëŒ€ê°’ + 1ë¶€í„° ì‹œì‘)
        date_counters[pk_date] += 1
        
        # ë‚ ì§œë³„ ê³ ìœ í•œ conv_id ìƒì„±
        conv_id = pk_date + '_' + str(date_counters[pk_date]).zfill(5)
        conv_ids.append(conv_id)
        
        # ë‚´ìš© ê¸°ë°˜ í•´ì‹œê°’ ìƒì„± (ì¤‘ë³µ ì²´í¬ìš©)
        # Qì™€ AëŠ” ê°™ì€ ëŒ€í™”ì´ë¯€ë¡œ user_id, date, contentë§Œìœ¼ë¡œ í•´ì‹œ ìƒì„±
        import hashlib
        content_hash = hashlib.md5(
            f"{input_data['user_id'][idx]}_{input_data['content'][idx]}_{input_data['date'][idx]}".encode()
        ).hexdigest()
        content_hashes.append(content_hash)
        
        # Q&A ìŒ ì—°ê²°ì„ ìœ„í•œ hash_ref ìƒì„±
        qa_type = input_data['q/a'][idx]
        user_id = input_data['user_id'][idx]
        date_key = input_data['date'][idx]
        
        # Q&A ìŒì„ êµ¬ë¶„í•˜ê¸° ìœ„í•œ í‚¤ ìƒì„± (user_id + date + ìˆœì„œ)
        qa_key = f"{user_id}_{date_key}_{idx//2}"  # 2ê°œì”© ìŒì´ë¯€ë¡œ idx//2ë¡œ ê·¸ë£¹í•‘
        
        if qa_type == 'Q':
            # Qì¸ ê²½ìš°: ìì‹ ì˜ í•´ì‹œê°’ì„ ì €ì¥í•˜ê³  hash_refëŠ” NULL
            qa_pairs[qa_key] = content_hash
            hash_refs.append(None)
        elif qa_type == 'A':
            # Aì¸ ê²½ìš°: í•´ë‹¹í•˜ëŠ” Qì˜ í•´ì‹œê°’ì„ hash_refë¡œ ì„¤ì •
            if qa_key in qa_pairs:
                hash_refs.append(qa_pairs[qa_key])
            else:
                # Që¥¼ ì°¾ì§€ ëª»í•œ ê²½ìš° (ë°ì´í„° ìˆœì„œ ë¬¸ì œ ë“±)
                hash_refs.append(None)
                logger.warning(f"Aì— ëŒ€ì‘í•˜ëŠ” Që¥¼ ì°¾ì§€ ëª»í•¨: {conv_id}")
        else:
            hash_refs.append(None)
    
    input_data.insert(0, 'conv_id', conv_ids)
    input_data.insert(1, 'hash_value', content_hashes)  # í•´ì‹œê°’ ì»¬ëŸ¼ ì¶”ê°€
    input_data.insert(2, 'hash_ref', hash_refs)  # Q&A ì—°ê²°ìš© hash_ref ì»¬ëŸ¼ ì¶”ê°€
    
    # ë””ë²„ê¹…: hash_ref ê°’ í™•ì¸
    print(f"ğŸ” hash_ref ê°’ ìƒ˜í”Œ (ì²˜ìŒ 5ê°œ): {hash_refs[:5]}")
    print(f"ğŸ” hash_value ê°’ ìƒ˜í”Œ (ì²˜ìŒ 5ê°œ): {content_hashes[:5]}")
    print(f"ğŸ” input_data ì»¬ëŸ¼ ìˆœì„œ: {list(input_data.columns)}")
    print(f"ğŸ” input_data shape: {input_data.shape}")
    
    # ì¤‘ë³µ ì €ì¥ ë°©ì§€ í†µê³„
    total_records = len(input_data)
    existing_records = 0
    new_records = 0
    
    for idx in tqdm(range(len(input_data))):   # PostgreSQL í…Œì´ë¸”ì— ë°ì´í„° ì €ì¥
        # í•´ì‹œê°’ ê¸°ì¤€ìœ¼ë¡œ ì¤‘ë³µ ì²´í¬
        if pipe.postgres.check_hash_duplicate(pipe.env_manager.conv_tb_name, input_data['hash_value'][idx]):
            existing_records += 1
            logger.info(f"ì´ë¯¸ ì¡´ì¬í•˜ëŠ” ë°ì´í„° (í•´ì‹œ: {input_data['hash_value'][idx][:8]}...): {input_data['conv_id'][idx]}")
            continue
        
        new_records += 1
        data_set = tuple(input_data.iloc[idx].values)
        
        # ë””ë²„ê¹…: ì €ì¥í•  ë°ì´í„° í™•ì¸ (ì²˜ìŒ 3ê°œë§Œ)
        if idx < 3:
            print(f"ğŸ” ì €ì¥í•  ë°ì´í„° {idx}: {data_set}")
            print(f"   - conv_id: {data_set[0]}")
            print(f"   - hash_value: {data_set[1]}")
            print(f"   - hash_ref: {data_set[2]}")
            print(f"   - q/a: {data_set[4]}")
        
        pipe.table_editor.edit_conv_table('insert', pipe.env_manager.conv_tb_name, data_type='raw', data=data_set)
    
    # ì €ì¥ ê²°ê³¼ ìš”ì•½
    summary_msg = f"ğŸ“Š ë°ì´í„° ì €ì¥ ì™„ë£Œ - ì „ì²´: {total_records}, ì‹ ê·œ: {new_records}, ì¤‘ë³µ: {existing_records}"
    print(f"\n{summary_msg}")
    print(f"   ì „ì²´ ë ˆì½”ë“œ: {total_records}")
    print(f"   ìƒˆë¡œ ì €ì¥ëœ ë ˆì½”ë“œ: {new_records}")
    print(f"   ì´ë¯¸ ì¡´ì¬í•˜ëŠ” ë ˆì½”ë“œ: {existing_records}")
    print(f"   ì¤‘ë³µë¥ : {(existing_records/total_records*100):.1f}%" if total_records > 0 else "   ì¤‘ë³µë¥ : 0%")
    
    logger.info(summary_msg)
    logger.info(f"âœ… ë°ì´í„° ìˆ˜ì§‘ ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ({datetime.now().strftime('%Y-%m-%d %H:%M:%S')})")
    pipe.postgres.db_connection.close()

if __name__ == '__main__':
    cli_parser = argparse.ArgumentParser()
    cli_parser.add_argument('--data_path', type=str, default='./data/')
    cli_parser.add_argument('--file_name', type=str, default='conv_log-0705-0902.xlsx')
    cli_parser.add_argument('--config_path', type=str, default='./config/')
    cli_parser.add_argument('--task_name', type=str, default='cls')
    cli_parser.add_argument('--process', type=str, default='daily', 
                           help='ì‹¤í–‰ ëª¨ë“œ: daily(ì¼íšŒì„±), scheduled(ìŠ¤ì¼€ì¤„ë§)')
    cli_parser.add_argument('--schedule_type', type=str, default='hourly',
                           help='ìŠ¤ì¼€ì¤„ íƒ€ì…: hourly, daily, every_30min, every_15min, business_hours')
    cli_parser.add_argument('--query', type=str, default=None)
    cli_args = cli_parser.parse_args()
    
    # ìŠ¤ì¼€ì¤„ë§ ëª¨ë“œ í™•ì¸
    if cli_args.process == 'scheduled':
        # ì‚¬ìš© ê°€ëŠ¥í•œ ìŠ¤ì¼€ì¤„ ì˜µì…˜ ì¶œë ¥
        print_available_schedules()
        
        # ìŠ¤ì¼€ì¤„ ì„¤ì • ê°€ì ¸ì˜¤ê¸°
        schedule_config = get_schedule_config(cli_args.schedule_type)
        print(f"\nâœ… ì„ íƒëœ ìŠ¤ì¼€ì¤„: {schedule_config['description']}")
        
        # APSchedulerë¥¼ ì‚¬ìš©í•œ ìŠ¤ì¼€ì¤„ë§
        scheduler = BackgroundScheduler()
        
        # ìŠ¤ì¼€ì¤„ëœ ì‘ì—… ì¶”ê°€
        scheduler.add_job(
            func=main,
            trigger=schedule_config['trigger'],
            args=[cli_args],
            id='data_collection_job',
            name=f'ë°ì´í„° ìˆ˜ì§‘ ({cli_args.schedule_type})',
            replace_existing=True,
            max_instances=1  # ë™ì‹œ ì‹¤í–‰ ë°©ì§€
        )
        
        # ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘
        scheduler.start()
        logger = logging.getLogger(__name__)
        logger.info(f"ğŸ• APSchedulerê°€ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤. {schedule_config['description']}")
        logger.info("ğŸ“… ì˜ˆì •ëœ ì‘ì—…ë“¤:")
        for job in scheduler.get_jobs():
            logger.info(f"   - {job.name}: {job.next_run_time}")
        
        # ìŠ¤ì¼€ì¤„ëŸ¬ ìƒíƒœ í™•ì¸ì„ ìœ„í•œ ì£¼ê¸°ì  ë¡œê·¸
        logger.info("â° ìŠ¤ì¼€ì¤„ëŸ¬ê°€ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤. ë§¤ì‹œê°„ 5ë¶„ì— ë°ì´í„° ìˆ˜ì§‘ì´ ì‹¤í–‰ë©ë‹ˆë‹¤.")
        
        try:
            # ë©”ì¸ ìŠ¤ë ˆë“œ ìœ ì§€ ë° ì£¼ê¸°ì  ìƒíƒœ í™•ì¸
            check_count = 0
            while True:
                time.sleep(60)  # 1ë¶„ë§ˆë‹¤ ì²´í¬
                check_count += 1
                
                # 10ë¶„ë§ˆë‹¤ ìŠ¤ì¼€ì¤„ëŸ¬ ìƒíƒœ ë¡œê·¸
                if check_count % 10 == 0:
                    jobs = scheduler.get_jobs()
                    if jobs:
                        next_run = jobs[0].next_run_time
                        logger.info(f"â° ìŠ¤ì¼€ì¤„ëŸ¬ ì‹¤í–‰ ì¤‘ - ë‹¤ìŒ ì‹¤í–‰ ì˜ˆì •: {next_run}")
                    else:
                        logger.warning("âš ï¸ ë“±ë¡ëœ ì‘ì—…ì´ ì—†ìŠµë‹ˆë‹¤!")
                        
        except KeyboardInterrupt:
            logger.info("â¹ï¸ ìŠ¤ì¼€ì¤„ëŸ¬ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤...")
            scheduler.shutdown()
    else:
        # ì¼íšŒì„± ì‹¤í–‰
        main(cli_args)